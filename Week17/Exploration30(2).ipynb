{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 : 다양한 조건의 음악 생성하기\n",
    "\n",
    "## Step 1. MAESTRO 데이터셋을 전처리하여 훈련용 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import concurrent.futures\n",
    "\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON = 1\n",
    "OFF = 0\n",
    "CC = 2\n",
    "\n",
    "current_time = 0\n",
    "eventlist = []\n",
    "cc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "CCDim = 2\n",
    "CCOffset = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim + CCDim # 390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, length):    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.80, 1.20)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim - 1)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = (d[3] / 128) * VelocityDim + VelocityOffset\n",
    "            note = d[2] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[2] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "        # CC\n",
    "        elif d[1] == 2:\n",
    "            event = CCOffset + d[3]\n",
    "            eventlist.append(event)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "    \n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME')+'/aiffel/music_transformer/data/midi_test.npy'\n",
    "\n",
    "get_midi = np.load(data_path, allow_pickle=True)\n",
    "get_midi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 256\n",
    "train = []\n",
    "labels = []\n",
    "\n",
    "for midi_list in get_midi:\n",
    "    cut_list = [midi_list[i:i+length] for i in range(0, len(midi_list), length)]\n",
    "    for sublist in cut_list:\n",
    "        x, y = get_data(np.array(sublist), length)\n",
    "        train.append(x)\n",
    "        labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59268, 256) (59268, 256)\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(train.shape, labels.shape)   # 학습을 위해 MIDI list를 256 길이로 나누었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train,\n",
    "                               maxlen=length,\n",
    "                               padding='post',\n",
    "                               value=0)\n",
    "train_label_pad = pad_sequences(labels,\n",
    "                                maxlen=length,\n",
    "                                padding='post',\n",
    "                                value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_casting(train, label):\n",
    "    train = tf.cast(train, tf.int64)\n",
    "    label = tf.cast(label, tf.int64)\n",
    "\n",
    "    return train, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "train_dataset = train_dataset.map(tensor_casting)\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Music Transformer 모델을 구현하여 학습 진행하기\n",
    "\n",
    "단, 20epoch를 완전히 학습 진행하지 않아도 좋습니다. 하지만 최초의 체크포인트가 저장되는 2epoch까지는 진행해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(RelativeGlobalAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.headDim = d_model // num_heads\n",
    "        self.contextDim = int(self.headDim * self.num_heads)\n",
    "        self.eventDim = 390\n",
    "        self.E = self.add_weight('E', shape=[self.num_heads, length, self.headDim])\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wk = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wv = tf.keras.layers.Dense(self.headDim)\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        q = tf.stack([self.wq(q) for _ in range(self.num_heads)])\n",
    "        k = tf.stack([self.wk(k) for _ in range(self.num_heads)])\n",
    "        v = tf.stack([self.wv(v) for _ in range(self.num_heads)])\n",
    "\n",
    "        self.batch_size = q.shape[1]\n",
    "        self.max_len = q.shape[2]\n",
    "        \n",
    "        #skewing\n",
    "        # E = Heads, Time, HeadDim\n",
    "        # [Heads, Batch * Time, HeadDim]\n",
    "        Q_ = tf.reshape(q, [self.num_heads, self.batch_size * self.max_len, self.headDim])\n",
    "        # [Heads, Batch * Time, Time]\n",
    "        S = tf.matmul(Q_, self.E, transpose_b=True)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len, self.max_len])\n",
    "        # [Heads, Batch, Time, Time+1]\n",
    "        S = tf.pad(S, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [Heads, Batch, Time+1, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len + 1, self.max_len])   \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = S[:, :, 1:]\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = (tf.matmul(q, k, transpose_b=True) + S) / np.sqrt(self.headDim)\n",
    "        # mask tf 2.0 == tf.linalg.band_part\n",
    "        get_mask = tf.linalg.band_part(tf.ones([self.max_len, self.max_len]), -1, 0)\n",
    "        attention = attention * get_mask - tf.cast(1e10, attention.dtype) * (1-get_mask)\n",
    "        score = tf.nn.softmax(attention, axis=3)\n",
    "\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        context = tf.matmul(score, v)\n",
    "        # [Batch, Time, Heads, HeadDim]\n",
    "        context = tf.transpose(context, [1, 2, 0, 3])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.reshape(context, [self.batch_size, self.max_len, self.d_model])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        logits = tf.keras.layers.Dense(self.d_model)(context)\n",
    "\n",
    "        return logits, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.rga = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.rga(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.rga1 = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.rga2 = RelativeGlobalAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.rga1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.rga2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attention_weights = {}\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inp, training, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "        embed = self.embedding(inp)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        enc_output = self.encoder(embed, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            embed, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 390   # MIDI가 낼 수 있는 소리의 종류\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "music_transformer = MusicTransformer(num_layers, d_model, num_heads, dff,\n",
    "                                     input_vocab_size, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.getenv('HOME')+'/aiffel/music_transformer/models/'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(music_transformer=music_transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.4321\n",
      "Epoch 1 Batch 50 Loss 3.3521\n",
      "Epoch 1 Batch 100 Loss 3.3383\n",
      "Epoch 1 Batch 150 Loss 3.3349\n",
      "Epoch 1 Batch 200 Loss 3.3384\n",
      "Epoch 1 Batch 250 Loss 3.3343\n",
      "Epoch 1 Batch 300 Loss 3.3339\n",
      "Epoch 1 Batch 350 Loss 3.3293\n",
      "Epoch 1 Batch 400 Loss 3.3280\n",
      "Epoch 1 Batch 450 Loss 3.3295\n",
      "Epoch 1 Batch 500 Loss 3.3310\n",
      "Epoch 1 Batch 550 Loss 3.3296\n",
      "Epoch 1 Batch 600 Loss 3.3294\n",
      "Epoch 1 Batch 650 Loss 3.3288\n",
      "Epoch 1 Batch 700 Loss 3.3286\n",
      "Epoch 1 Batch 750 Loss 3.3297\n",
      "Epoch 1 Batch 800 Loss 3.3303\n",
      "Epoch 1 Batch 850 Loss 3.3304\n",
      "Epoch 1 Batch 900 Loss 3.3301\n",
      "Epoch 1 Batch 950 Loss 3.3303\n",
      "Epoch 1 Batch 1000 Loss 3.3309\n",
      "Epoch 1 Batch 1050 Loss 3.3322\n",
      "Epoch 1 Batch 1100 Loss 3.3326\n",
      "Epoch 1 Batch 1150 Loss 3.3328\n",
      "Epoch 1 Batch 1200 Loss 3.3324\n",
      "Epoch 1 Batch 1250 Loss 3.3316\n",
      "Epoch 1 Batch 1300 Loss 3.3304\n",
      "Epoch 1 Batch 1350 Loss 3.3303\n",
      "Epoch 1 Batch 1400 Loss 3.3311\n",
      "Epoch 1 Batch 1450 Loss 3.3313\n",
      "Epoch 1 Batch 1500 Loss 3.3309\n",
      "Epoch 1 Batch 1550 Loss 3.3309\n",
      "Epoch 1 Batch 1600 Loss 3.3305\n",
      "Epoch 1 Batch 1650 Loss 3.3307\n",
      "Epoch 1 Batch 1700 Loss 3.3309\n",
      "Epoch 1 Batch 1750 Loss 3.3309\n",
      "Epoch 1 Batch 1800 Loss 3.3311\n",
      "Epoch 1 Batch 1850 Loss 3.3310\n",
      "Epoch 1 Batch 1900 Loss 3.3306\n",
      "Epoch 1 Batch 1950 Loss 3.3307\n",
      "Epoch 1 Batch 2000 Loss 3.3310\n",
      "Epoch 1 Batch 2050 Loss 3.3309\n",
      "Epoch 1 Batch 2100 Loss 3.3309\n",
      "Epoch 1 Batch 2150 Loss 3.3313\n",
      "Epoch 1 Batch 2200 Loss 3.3317\n",
      "Epoch 1 Batch 2250 Loss 3.3316\n",
      "Epoch 1 Batch 2300 Loss 3.3317\n",
      "Epoch 1 Batch 2350 Loss 3.3317\n",
      "Epoch 1 Batch 2400 Loss 3.3317\n",
      "Epoch 1 Batch 2450 Loss 3.3314\n",
      "Epoch 1 Batch 2500 Loss 3.3311\n",
      "Epoch 1 Batch 2550 Loss 3.3308\n",
      "Epoch 1 Batch 2600 Loss 3.3308\n",
      "Epoch 1 Batch 2650 Loss 3.3311\n",
      "Epoch 1 Batch 2700 Loss 3.3309\n",
      "Epoch 1 Batch 2750 Loss 3.3311\n",
      "Epoch 1 Batch 2800 Loss 3.3310\n",
      "Epoch 1 Batch 2850 Loss 3.3311\n",
      "Epoch 1 Batch 2900 Loss 3.3311\n",
      "Epoch 1 Batch 2950 Loss 3.3310\n",
      "Epoch 1 Batch 3000 Loss 3.3312\n",
      "Epoch 1 Batch 3050 Loss 3.3312\n",
      "Epoch 1 Batch 3100 Loss 3.3312\n",
      "Epoch 1 Batch 3150 Loss 3.3311\n",
      "Epoch 1 Batch 3200 Loss 3.3310\n",
      "Epoch 1 Batch 3250 Loss 3.3310\n",
      "Epoch 1 Batch 3300 Loss 3.3310\n",
      "Epoch 1 Batch 3350 Loss 3.3308\n",
      "Epoch 1 Batch 3400 Loss 3.3307\n",
      "Epoch 1 Batch 3450 Loss 3.3305\n",
      "Epoch 1 Batch 3500 Loss 3.3306\n",
      "Epoch 1 Batch 3550 Loss 3.3310\n",
      "Epoch 1 Batch 3600 Loss 3.3312\n",
      "Epoch 1 Batch 3650 Loss 3.3315\n",
      "Epoch 1 Batch 3700 Loss 3.3319\n",
      "Epoch 1 Batch 3750 Loss 3.3322\n",
      "Epoch 1 Batch 3800 Loss 3.3326\n",
      "Epoch 1 Batch 3850 Loss 3.3326\n",
      "Epoch 1 Batch 3900 Loss 3.3326\n",
      "Epoch 1 Batch 3950 Loss 3.3328\n",
      "Epoch 1 Batch 4000 Loss 3.3325\n",
      "Epoch 1 Batch 4050 Loss 3.3324\n",
      "Epoch 1 Batch 4100 Loss 3.3325\n",
      "Epoch 1 Batch 4150 Loss 3.3326\n",
      "Epoch 1 Batch 4200 Loss 3.3324\n",
      "Epoch 1 Batch 4250 Loss 3.3324\n",
      "Epoch 1 Batch 4300 Loss 3.3324\n",
      "Epoch 1 Batch 4350 Loss 3.3322\n",
      "Epoch 1 Batch 4400 Loss 3.3322\n",
      "Epoch 1 Batch 4450 Loss 3.3323\n",
      "Epoch 1 Batch 4500 Loss 3.3324\n",
      "Epoch 1 Batch 4550 Loss 3.3326\n",
      "Epoch 1 Batch 4600 Loss 3.3325\n",
      "Epoch 1 Batch 4650 Loss 3.3324\n",
      "Epoch 1 Batch 4700 Loss 3.3326\n",
      "Epoch 1 Batch 4750 Loss 3.3326\n",
      "Epoch 1 Batch 4800 Loss 3.3326\n",
      "Epoch 1 Batch 4850 Loss 3.3328\n",
      "Epoch 1 Batch 4900 Loss 3.3327\n",
      "Epoch 1 Batch 4950 Loss 3.3327\n",
      "Epoch 1 Batch 5000 Loss 3.3326\n",
      "Epoch 1 Batch 5050 Loss 3.3325\n",
      "Epoch 1 Batch 5100 Loss 3.3326\n",
      "Epoch 1 Batch 5150 Loss 3.3328\n",
      "Epoch 1 Batch 5200 Loss 3.3328\n",
      "Epoch 1 Batch 5250 Loss 3.3327\n",
      "Epoch 1 Batch 5300 Loss 3.3327\n",
      "Epoch 1 Batch 5350 Loss 3.3327\n",
      "Epoch 1 Batch 5400 Loss 3.3329\n",
      "Epoch 1 Batch 5450 Loss 3.3331\n",
      "Epoch 1 Batch 5500 Loss 3.3332\n",
      "Epoch 1 Batch 5550 Loss 3.3333\n",
      "Epoch 1 Batch 5600 Loss 3.3333\n",
      "Epoch 1 Batch 5650 Loss 3.3334\n",
      "Epoch 1 Batch 5700 Loss 3.3335\n",
      "Epoch 1 Batch 5750 Loss 3.3334\n",
      "Epoch 1 Batch 5800 Loss 3.3334\n",
      "Epoch 1 Batch 5850 Loss 3.3334\n",
      "Epoch 1 Batch 5900 Loss 3.3334\n",
      "Epoch 1 Batch 5950 Loss 3.3334\n",
      "Epoch 1 Batch 6000 Loss 3.3333\n",
      "Epoch 1 Batch 6050 Loss 3.3334\n",
      "Epoch 1 Batch 6100 Loss 3.3334\n",
      "Epoch 1 Batch 6150 Loss 3.3334\n",
      "Epoch 1 Batch 6200 Loss 3.3334\n",
      "Epoch 1 Batch 6250 Loss 3.3334\n",
      "Epoch 1 Batch 6300 Loss 3.3334\n",
      "Epoch 1 Batch 6350 Loss 3.3334\n",
      "Epoch 1 Batch 6400 Loss 3.3335\n",
      "Epoch 1 Batch 6450 Loss 3.3335\n",
      "Epoch 1 Batch 6500 Loss 3.3334\n",
      "Epoch 1 Batch 6550 Loss 3.3334\n",
      "Epoch 1 Batch 6600 Loss 3.3333\n",
      "Epoch 1 Batch 6650 Loss 3.3333\n",
      "Epoch 1 Batch 6700 Loss 3.3333\n",
      "Epoch 1 Batch 6750 Loss 3.3333\n",
      "Epoch 1 Batch 6800 Loss 3.3333\n",
      "Epoch 1 Batch 6850 Loss 3.3333\n",
      "Epoch 1 Batch 6900 Loss 3.3332\n",
      "Epoch 1 Batch 6950 Loss 3.3331\n",
      "Epoch 1 Batch 7000 Loss 3.3331\n",
      "Epoch 1 Batch 7050 Loss 3.3330\n",
      "Epoch 1 Batch 7100 Loss 3.3329\n",
      "Epoch 1 Batch 7150 Loss 3.3328\n",
      "Epoch 1 Batch 7200 Loss 3.3328\n",
      "Epoch 1 Batch 7250 Loss 3.3327\n",
      "Epoch 1 Batch 7300 Loss 3.3326\n",
      "Epoch 1 Batch 7350 Loss 3.3325\n",
      "Epoch 1 Batch 7400 Loss 3.3326\n",
      "Epoch 1 Loss 3.3326\n",
      "Time taken for 1 epoch: 2257.7605686187744 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.3930\n",
      "Epoch 2 Batch 50 Loss 3.3338\n",
      "Epoch 2 Batch 100 Loss 3.3181\n",
      "Epoch 2 Batch 150 Loss 3.3203\n",
      "Epoch 2 Batch 200 Loss 3.3183\n",
      "Epoch 2 Batch 250 Loss 3.3167\n",
      "Epoch 2 Batch 300 Loss 3.3183\n",
      "Epoch 2 Batch 350 Loss 3.3183\n",
      "Epoch 2 Batch 400 Loss 3.3193\n",
      "Epoch 2 Batch 450 Loss 3.3185\n",
      "Epoch 2 Batch 500 Loss 3.3188\n",
      "Epoch 2 Batch 550 Loss 3.3183\n",
      "Epoch 2 Batch 600 Loss 3.3178\n",
      "Epoch 2 Batch 650 Loss 3.3182\n",
      "Epoch 2 Batch 700 Loss 3.3167\n",
      "Epoch 2 Batch 750 Loss 3.3170\n",
      "Epoch 2 Batch 800 Loss 3.3176\n",
      "Epoch 2 Batch 850 Loss 3.3172\n",
      "Epoch 2 Batch 900 Loss 3.3180\n",
      "Epoch 2 Batch 950 Loss 3.3187\n",
      "Epoch 2 Batch 1000 Loss 3.3186\n",
      "Epoch 2 Batch 1050 Loss 3.3186\n",
      "Epoch 2 Batch 1100 Loss 3.3188\n",
      "Epoch 2 Batch 1150 Loss 3.3200\n",
      "Epoch 2 Batch 1200 Loss 3.3197\n",
      "Epoch 2 Batch 1250 Loss 3.3191\n",
      "Epoch 2 Batch 1300 Loss 3.3189\n",
      "Epoch 2 Batch 1350 Loss 3.3198\n",
      "Epoch 2 Batch 1400 Loss 3.3194\n",
      "Epoch 2 Batch 1450 Loss 3.3195\n",
      "Epoch 2 Batch 1500 Loss 3.3196\n",
      "Epoch 2 Batch 1550 Loss 3.3196\n",
      "Epoch 2 Batch 1600 Loss 3.3193\n",
      "Epoch 2 Batch 1650 Loss 3.3192\n",
      "Epoch 2 Batch 1700 Loss 3.3187\n",
      "Epoch 2 Batch 1750 Loss 3.3187\n",
      "Epoch 2 Batch 1800 Loss 3.3192\n",
      "Epoch 2 Batch 1850 Loss 3.3193\n",
      "Epoch 2 Batch 1900 Loss 3.3192\n",
      "Epoch 2 Batch 1950 Loss 3.3198\n",
      "Epoch 2 Batch 2000 Loss 3.3194\n",
      "Epoch 2 Batch 2050 Loss 3.3195\n",
      "Epoch 2 Batch 2100 Loss 3.3191\n",
      "Epoch 2 Batch 2150 Loss 3.3191\n",
      "Epoch 2 Batch 2200 Loss 3.3193\n",
      "Epoch 2 Batch 2250 Loss 3.3197\n",
      "Epoch 2 Batch 2300 Loss 3.3193\n",
      "Epoch 2 Batch 2350 Loss 3.3191\n",
      "Epoch 2 Batch 2400 Loss 3.3188\n",
      "Epoch 2 Batch 2450 Loss 3.3186\n",
      "Epoch 2 Batch 2500 Loss 3.3188\n",
      "Epoch 2 Batch 2550 Loss 3.3189\n",
      "Epoch 2 Batch 2600 Loss 3.3185\n",
      "Epoch 2 Batch 2650 Loss 3.3189\n",
      "Epoch 2 Batch 2700 Loss 3.3190\n",
      "Epoch 2 Batch 2750 Loss 3.3192\n",
      "Epoch 2 Batch 2800 Loss 3.3196\n",
      "Epoch 2 Batch 2850 Loss 3.3202\n",
      "Epoch 2 Batch 2900 Loss 3.3204\n",
      "Epoch 2 Batch 2950 Loss 3.3205\n",
      "Epoch 2 Batch 3000 Loss 3.3203\n",
      "Epoch 2 Batch 3050 Loss 3.3203\n",
      "Epoch 2 Batch 3100 Loss 3.3204\n",
      "Epoch 2 Batch 3150 Loss 3.3204\n",
      "Epoch 2 Batch 3200 Loss 3.3202\n",
      "Epoch 2 Batch 3250 Loss 3.3204\n",
      "Epoch 2 Batch 3300 Loss 3.3204\n",
      "Epoch 2 Batch 3350 Loss 3.3203\n",
      "Epoch 2 Batch 3400 Loss 3.3202\n",
      "Epoch 2 Batch 3450 Loss 3.3202\n",
      "Epoch 2 Batch 3500 Loss 3.3200\n",
      "Epoch 2 Batch 3550 Loss 3.3200\n",
      "Epoch 2 Batch 3600 Loss 3.3202\n",
      "Epoch 2 Batch 3650 Loss 3.3203\n",
      "Epoch 2 Batch 3700 Loss 3.3205\n",
      "Epoch 2 Batch 3750 Loss 3.3209\n",
      "Epoch 2 Batch 3800 Loss 3.3210\n",
      "Epoch 2 Batch 3850 Loss 3.3211\n",
      "Epoch 2 Batch 3900 Loss 3.3212\n",
      "Epoch 2 Batch 3950 Loss 3.3214\n",
      "Epoch 2 Batch 4000 Loss 3.3215\n",
      "Epoch 2 Batch 4050 Loss 3.3216\n",
      "Epoch 2 Batch 4100 Loss 3.3219\n",
      "Epoch 2 Batch 4150 Loss 3.3219\n",
      "Epoch 2 Batch 4200 Loss 3.3220\n",
      "Epoch 2 Batch 4250 Loss 3.3221\n",
      "Epoch 2 Batch 4300 Loss 3.3222\n",
      "Epoch 2 Batch 4350 Loss 3.3222\n",
      "Epoch 2 Batch 4400 Loss 3.3222\n",
      "Epoch 2 Batch 4450 Loss 3.3224\n",
      "Epoch 2 Batch 4500 Loss 3.3223\n",
      "Epoch 2 Batch 4550 Loss 3.3222\n",
      "Epoch 2 Batch 4600 Loss 3.3223\n",
      "Epoch 2 Batch 4650 Loss 3.3223\n",
      "Epoch 2 Batch 4700 Loss 3.3225\n",
      "Epoch 2 Batch 4750 Loss 3.3224\n",
      "Epoch 2 Batch 4800 Loss 3.3226\n",
      "Epoch 2 Batch 4850 Loss 3.3227\n",
      "Epoch 2 Batch 4900 Loss 3.3230\n",
      "Epoch 2 Batch 4950 Loss 3.3230\n",
      "Epoch 2 Batch 5000 Loss 3.3229\n",
      "Epoch 2 Batch 5050 Loss 3.3231\n",
      "Epoch 2 Batch 5100 Loss 3.3231\n",
      "Epoch 2 Batch 5150 Loss 3.3232\n",
      "Epoch 2 Batch 5200 Loss 3.3232\n",
      "Epoch 2 Batch 5250 Loss 3.3232\n",
      "Epoch 2 Batch 5300 Loss 3.3231\n",
      "Epoch 2 Batch 5350 Loss 3.3232\n",
      "Epoch 2 Batch 5400 Loss 3.3235\n",
      "Epoch 2 Batch 5450 Loss 3.3233\n",
      "Epoch 2 Batch 5500 Loss 3.3232\n",
      "Epoch 2 Batch 5550 Loss 3.3232\n",
      "Epoch 2 Batch 5600 Loss 3.3232\n",
      "Epoch 2 Batch 5650 Loss 3.3232\n",
      "Epoch 2 Batch 5700 Loss 3.3233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 5750 Loss 3.3234\n",
      "Epoch 2 Batch 5800 Loss 3.3232\n",
      "Epoch 2 Batch 5850 Loss 3.3233\n",
      "Epoch 2 Batch 5900 Loss 3.3231\n",
      "Epoch 2 Batch 5950 Loss 3.3231\n",
      "Epoch 2 Batch 6000 Loss 3.3231\n",
      "Epoch 2 Batch 6050 Loss 3.3233\n",
      "Epoch 2 Batch 6100 Loss 3.3232\n",
      "Epoch 2 Batch 6150 Loss 3.3231\n",
      "Epoch 2 Batch 6200 Loss 3.3232\n",
      "Epoch 2 Batch 6250 Loss 3.3231\n",
      "Epoch 2 Batch 6300 Loss 3.3232\n",
      "Epoch 2 Batch 6350 Loss 3.3231\n",
      "Epoch 2 Batch 6400 Loss 3.3231\n",
      "Epoch 2 Batch 6450 Loss 3.3231\n",
      "Epoch 2 Batch 6500 Loss 3.3230\n",
      "Epoch 2 Batch 6550 Loss 3.3229\n",
      "Epoch 2 Batch 6600 Loss 3.3230\n",
      "Epoch 2 Batch 6650 Loss 3.3229\n",
      "Epoch 2 Batch 6700 Loss 3.3230\n",
      "Epoch 2 Batch 6750 Loss 3.3230\n",
      "Epoch 2 Batch 6800 Loss 3.3229\n",
      "Epoch 2 Batch 6850 Loss 3.3230\n",
      "Epoch 2 Batch 6900 Loss 3.3231\n",
      "Epoch 2 Batch 6950 Loss 3.3231\n",
      "Epoch 2 Batch 7000 Loss 3.3231\n",
      "Epoch 2 Batch 7050 Loss 3.3231\n",
      "Epoch 2 Batch 7100 Loss 3.3231\n",
      "Epoch 2 Batch 7150 Loss 3.3232\n",
      "Epoch 2 Batch 7200 Loss 3.3231\n",
      "Epoch 2 Batch 7250 Loss 3.3231\n",
      "Epoch 2 Batch 7300 Loss 3.3230\n",
      "Epoch 2 Batch 7350 Loss 3.3230\n",
      "Epoch 2 Batch 7400 Loss 3.3230\n",
      "Saving checkpoint for epoch 2 at /home/aiffel0039/aiffel/music_transformer/models/ckpt-11\n",
      "Epoch 2 Loss 3.3230\n",
      "Time taken for 1 epoch: 2211.521773338318 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.2166\n",
      "Epoch 3 Batch 50 Loss 3.2963\n",
      "Epoch 3 Batch 100 Loss 3.3169\n",
      "Epoch 3 Batch 150 Loss 3.3149\n",
      "Epoch 3 Batch 200 Loss 3.3134\n",
      "Epoch 3 Batch 250 Loss 3.3147\n",
      "Epoch 3 Batch 300 Loss 3.3139\n",
      "Epoch 3 Batch 350 Loss 3.3118\n",
      "Epoch 3 Batch 400 Loss 3.3118\n",
      "Epoch 3 Batch 450 Loss 3.3118\n",
      "Epoch 3 Batch 500 Loss 3.3080\n",
      "Epoch 3 Batch 550 Loss 3.3068\n",
      "Epoch 3 Batch 600 Loss 3.3053\n",
      "Epoch 3 Batch 650 Loss 3.3059\n",
      "Epoch 3 Batch 700 Loss 3.3046\n",
      "Epoch 3 Batch 750 Loss 3.3040\n",
      "Epoch 3 Batch 800 Loss 3.3054\n",
      "Epoch 3 Batch 850 Loss 3.3046\n",
      "Epoch 3 Batch 900 Loss 3.3057\n",
      "Epoch 3 Batch 950 Loss 3.3066\n",
      "Epoch 3 Batch 1000 Loss 3.3067\n",
      "Epoch 3 Batch 1050 Loss 3.3066\n",
      "Epoch 3 Batch 1100 Loss 3.3072\n",
      "Epoch 3 Batch 1150 Loss 3.3083\n",
      "Epoch 3 Batch 1200 Loss 3.3080\n",
      "Epoch 3 Batch 1250 Loss 3.3088\n",
      "Epoch 3 Batch 1300 Loss 3.3094\n",
      "Epoch 3 Batch 1350 Loss 3.3098\n",
      "Epoch 3 Batch 1400 Loss 3.3092\n",
      "Epoch 3 Batch 1450 Loss 3.3099\n",
      "Epoch 3 Batch 1500 Loss 3.3107\n",
      "Epoch 3 Batch 1550 Loss 3.3109\n",
      "Epoch 3 Batch 1600 Loss 3.3110\n",
      "Epoch 3 Batch 1650 Loss 3.3108\n",
      "Epoch 3 Batch 1700 Loss 3.3108\n",
      "Epoch 3 Batch 1750 Loss 3.3109\n",
      "Epoch 3 Batch 1800 Loss 3.3110\n",
      "Epoch 3 Batch 1850 Loss 3.3117\n",
      "Epoch 3 Batch 1900 Loss 3.3116\n",
      "Epoch 3 Batch 1950 Loss 3.3115\n",
      "Epoch 3 Batch 2000 Loss 3.3117\n",
      "Epoch 3 Batch 2050 Loss 3.3115\n",
      "Epoch 3 Batch 2100 Loss 3.3115\n",
      "Epoch 3 Batch 2150 Loss 3.3118\n",
      "Epoch 3 Batch 2200 Loss 3.3118\n",
      "Epoch 3 Batch 2250 Loss 3.3122\n",
      "Epoch 3 Batch 2300 Loss 3.3124\n",
      "Epoch 3 Batch 2350 Loss 3.3121\n",
      "Epoch 3 Batch 2400 Loss 3.3119\n",
      "Epoch 3 Batch 2450 Loss 3.3118\n",
      "Epoch 3 Batch 2500 Loss 3.3118\n",
      "Epoch 3 Batch 2550 Loss 3.3118\n",
      "Epoch 3 Batch 2600 Loss 3.3118\n",
      "Epoch 3 Batch 2650 Loss 3.3117\n",
      "Epoch 3 Batch 2700 Loss 3.3116\n",
      "Epoch 3 Batch 2750 Loss 3.3115\n",
      "Epoch 3 Batch 2800 Loss 3.3116\n",
      "Epoch 3 Batch 2850 Loss 3.3118\n",
      "Epoch 3 Batch 2900 Loss 3.3123\n",
      "Epoch 3 Batch 2950 Loss 3.3127\n",
      "Epoch 3 Batch 3000 Loss 3.3125\n",
      "Epoch 3 Batch 3050 Loss 3.3123\n",
      "Epoch 3 Batch 3100 Loss 3.3125\n",
      "Epoch 3 Batch 3150 Loss 3.3126\n",
      "Epoch 3 Batch 3200 Loss 3.3128\n",
      "Epoch 3 Batch 3250 Loss 3.3127\n",
      "Epoch 3 Batch 3300 Loss 3.3128\n",
      "Epoch 3 Batch 3350 Loss 3.3127\n",
      "Epoch 3 Batch 3400 Loss 3.3128\n",
      "Epoch 3 Batch 3450 Loss 3.3127\n",
      "Epoch 3 Batch 3500 Loss 3.3128\n",
      "Epoch 3 Batch 3550 Loss 3.3128\n",
      "Epoch 3 Batch 3600 Loss 3.3131\n",
      "Epoch 3 Batch 3650 Loss 3.3132\n",
      "Epoch 3 Batch 3700 Loss 3.3133\n",
      "Epoch 3 Batch 3750 Loss 3.3135\n",
      "Epoch 3 Batch 3800 Loss 3.3135\n",
      "Epoch 3 Batch 3850 Loss 3.3136\n",
      "Epoch 3 Batch 3900 Loss 3.3138\n",
      "Epoch 3 Batch 3950 Loss 3.3138\n",
      "Epoch 3 Batch 4000 Loss 3.3137\n",
      "Epoch 3 Batch 4050 Loss 3.3137\n",
      "Epoch 3 Batch 4100 Loss 3.3140\n",
      "Epoch 3 Batch 4150 Loss 3.3141\n",
      "Epoch 3 Batch 4200 Loss 3.3141\n",
      "Epoch 3 Batch 4250 Loss 3.3141\n",
      "Epoch 3 Batch 4300 Loss 3.3142\n",
      "Epoch 3 Batch 4350 Loss 3.3140\n",
      "Epoch 3 Batch 4400 Loss 3.3139\n",
      "Epoch 3 Batch 4450 Loss 3.3138\n",
      "Epoch 3 Batch 4500 Loss 3.3138\n",
      "Epoch 3 Batch 4550 Loss 3.3138\n",
      "Epoch 3 Batch 4600 Loss 3.3138\n",
      "Epoch 3 Batch 4650 Loss 3.3137\n",
      "Epoch 3 Batch 4700 Loss 3.3137\n",
      "Epoch 3 Batch 4750 Loss 3.3135\n",
      "Epoch 3 Batch 4800 Loss 3.3137\n",
      "Epoch 3 Batch 4850 Loss 3.3137\n",
      "Epoch 3 Batch 4900 Loss 3.3137\n",
      "Epoch 3 Batch 4950 Loss 3.3138\n",
      "Epoch 3 Batch 5000 Loss 3.3137\n",
      "Epoch 3 Batch 5050 Loss 3.3138\n",
      "Epoch 3 Batch 5100 Loss 3.3138\n",
      "Epoch 3 Batch 5150 Loss 3.3138\n",
      "Epoch 3 Batch 5200 Loss 3.3139\n",
      "Epoch 3 Batch 5250 Loss 3.3140\n",
      "Epoch 3 Batch 5300 Loss 3.3142\n",
      "Epoch 3 Batch 5350 Loss 3.3143\n",
      "Epoch 3 Batch 5400 Loss 3.3143\n",
      "Epoch 3 Batch 5450 Loss 3.3144\n",
      "Epoch 3 Batch 5500 Loss 3.3145\n",
      "Epoch 3 Batch 5550 Loss 3.3144\n",
      "Epoch 3 Batch 5600 Loss 3.3143\n",
      "Epoch 3 Batch 5650 Loss 3.3144\n",
      "Epoch 3 Batch 5700 Loss 3.3145\n",
      "Epoch 3 Batch 5750 Loss 3.3143\n",
      "Epoch 3 Batch 5800 Loss 3.3143\n",
      "Epoch 3 Batch 5850 Loss 3.3143\n",
      "Epoch 3 Batch 5900 Loss 3.3144\n",
      "Epoch 3 Batch 5950 Loss 3.3143\n",
      "Epoch 3 Batch 6000 Loss 3.3145\n",
      "Epoch 3 Batch 6050 Loss 3.3146\n",
      "Epoch 3 Batch 6100 Loss 3.3146\n",
      "Epoch 3 Batch 6150 Loss 3.3145\n",
      "Epoch 3 Batch 6200 Loss 3.3144\n",
      "Epoch 3 Batch 6250 Loss 3.3145\n",
      "Epoch 3 Batch 6300 Loss 3.3145\n",
      "Epoch 3 Batch 6350 Loss 3.3145\n",
      "Epoch 3 Batch 6400 Loss 3.3145\n",
      "Epoch 3 Batch 6450 Loss 3.3144\n",
      "Epoch 3 Batch 6500 Loss 3.3144\n",
      "Epoch 3 Batch 6550 Loss 3.3144\n",
      "Epoch 3 Batch 6600 Loss 3.3146\n",
      "Epoch 3 Batch 6650 Loss 3.3146\n",
      "Epoch 3 Batch 6700 Loss 3.3145\n",
      "Epoch 3 Batch 6750 Loss 3.3146\n",
      "Epoch 3 Batch 6800 Loss 3.3146\n",
      "Epoch 3 Batch 6850 Loss 3.3145\n",
      "Epoch 3 Batch 6900 Loss 3.3145\n",
      "Epoch 3 Batch 6950 Loss 3.3145\n",
      "Epoch 3 Batch 7000 Loss 3.3144\n",
      "Epoch 3 Batch 7050 Loss 3.3144\n",
      "Epoch 3 Batch 7100 Loss 3.3144\n",
      "Epoch 3 Batch 7150 Loss 3.3144\n",
      "Epoch 3 Batch 7200 Loss 3.3143\n",
      "Epoch 3 Batch 7250 Loss 3.3144\n",
      "Epoch 3 Batch 7300 Loss 3.3144\n",
      "Epoch 3 Batch 7350 Loss 3.3145\n",
      "Epoch 3 Batch 7400 Loss 3.3145\n",
      "Epoch 3 Loss 3.3144\n",
      "Time taken for 1 epoch: 2183.457343816757 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.3372\n",
      "Epoch 4 Batch 50 Loss 3.3138\n",
      "Epoch 4 Batch 100 Loss 3.3044\n",
      "Epoch 4 Batch 150 Loss 3.3033\n",
      "Epoch 4 Batch 200 Loss 3.3000\n",
      "Epoch 4 Batch 250 Loss 3.3004\n",
      "Epoch 4 Batch 300 Loss 3.3009\n",
      "Epoch 4 Batch 350 Loss 3.3016\n",
      "Epoch 4 Batch 400 Loss 3.3014\n",
      "Epoch 4 Batch 450 Loss 3.3007\n",
      "Epoch 4 Batch 500 Loss 3.2989\n",
      "Epoch 4 Batch 550 Loss 3.2965\n",
      "Epoch 4 Batch 600 Loss 3.2975\n",
      "Epoch 4 Batch 650 Loss 3.2978\n",
      "Epoch 4 Batch 700 Loss 3.2978\n",
      "Epoch 4 Batch 750 Loss 3.2980\n",
      "Epoch 4 Batch 800 Loss 3.2969\n",
      "Epoch 4 Batch 850 Loss 3.2979\n",
      "Epoch 4 Batch 900 Loss 3.2988\n",
      "Epoch 4 Batch 950 Loss 3.2994\n",
      "Epoch 4 Batch 1000 Loss 3.2997\n",
      "Epoch 4 Batch 1050 Loss 3.3002\n",
      "Epoch 4 Batch 1100 Loss 3.3006\n",
      "Epoch 4 Batch 1150 Loss 3.3013\n",
      "Epoch 4 Batch 1200 Loss 3.3016\n",
      "Epoch 4 Batch 1250 Loss 3.3015\n",
      "Epoch 4 Batch 1300 Loss 3.3022\n",
      "Epoch 4 Batch 1350 Loss 3.3020\n",
      "Epoch 4 Batch 1400 Loss 3.3022\n",
      "Epoch 4 Batch 1450 Loss 3.3021\n",
      "Epoch 4 Batch 1500 Loss 3.3025\n",
      "Epoch 4 Batch 1550 Loss 3.3028\n",
      "Epoch 4 Batch 1600 Loss 3.3029\n",
      "Epoch 4 Batch 1650 Loss 3.3032\n",
      "Epoch 4 Batch 1700 Loss 3.3030\n",
      "Epoch 4 Batch 1750 Loss 3.3029\n",
      "Epoch 4 Batch 1800 Loss 3.3028\n",
      "Epoch 4 Batch 1850 Loss 3.3027\n",
      "Epoch 4 Batch 1900 Loss 3.3026\n",
      "Epoch 4 Batch 1950 Loss 3.3024\n",
      "Epoch 4 Batch 2000 Loss 3.3021\n",
      "Epoch 4 Batch 2050 Loss 3.3018\n",
      "Epoch 4 Batch 2100 Loss 3.3017\n",
      "Epoch 4 Batch 2150 Loss 3.3018\n",
      "Epoch 4 Batch 2200 Loss 3.3017\n",
      "Epoch 4 Batch 2250 Loss 3.3018\n",
      "Epoch 4 Batch 2300 Loss 3.3019\n",
      "Epoch 4 Batch 2350 Loss 3.3021\n",
      "Epoch 4 Batch 2400 Loss 3.3024\n",
      "Epoch 4 Batch 2450 Loss 3.3021\n",
      "Epoch 4 Batch 2500 Loss 3.3025\n",
      "Epoch 4 Batch 2550 Loss 3.3022\n",
      "Epoch 4 Batch 2600 Loss 3.3024\n",
      "Epoch 4 Batch 2650 Loss 3.3026\n",
      "Epoch 4 Batch 2700 Loss 3.3027\n",
      "Epoch 4 Batch 2750 Loss 3.3028\n",
      "Epoch 4 Batch 2800 Loss 3.3026\n",
      "Epoch 4 Batch 2850 Loss 3.3028\n",
      "Epoch 4 Batch 2900 Loss 3.3028\n",
      "Epoch 4 Batch 2950 Loss 3.3029\n",
      "Epoch 4 Batch 3000 Loss 3.3029\n",
      "Epoch 4 Batch 3050 Loss 3.3031\n",
      "Epoch 4 Batch 3100 Loss 3.3033\n",
      "Epoch 4 Batch 3150 Loss 3.3034\n",
      "Epoch 4 Batch 3200 Loss 3.3035\n",
      "Epoch 4 Batch 3250 Loss 3.3034\n",
      "Epoch 4 Batch 3300 Loss 3.3035\n",
      "Epoch 4 Batch 3350 Loss 3.3034\n",
      "Epoch 4 Batch 3400 Loss 3.3035\n",
      "Epoch 4 Batch 3450 Loss 3.3033\n",
      "Epoch 4 Batch 3500 Loss 3.3034\n",
      "Epoch 4 Batch 3550 Loss 3.3036\n",
      "Epoch 4 Batch 3600 Loss 3.3039\n",
      "Epoch 4 Batch 3650 Loss 3.3042\n",
      "Epoch 4 Batch 3700 Loss 3.3045\n",
      "Epoch 4 Batch 3750 Loss 3.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 3800 Loss 3.3049\n",
      "Epoch 4 Batch 3850 Loss 3.3051\n",
      "Epoch 4 Batch 3900 Loss 3.3053\n",
      "Epoch 4 Batch 3950 Loss 3.3056\n",
      "Epoch 4 Batch 4000 Loss 3.3056\n",
      "Epoch 4 Batch 4050 Loss 3.3057\n",
      "Epoch 4 Batch 4100 Loss 3.3059\n",
      "Epoch 4 Batch 4150 Loss 3.3058\n",
      "Epoch 4 Batch 4200 Loss 3.3060\n",
      "Epoch 4 Batch 4250 Loss 3.3059\n",
      "Epoch 4 Batch 4300 Loss 3.3059\n",
      "Epoch 4 Batch 4350 Loss 3.3059\n",
      "Epoch 4 Batch 4400 Loss 3.3059\n",
      "Epoch 4 Batch 4450 Loss 3.3059\n",
      "Epoch 4 Batch 4500 Loss 3.3058\n",
      "Epoch 4 Batch 4550 Loss 3.3057\n",
      "Epoch 4 Batch 4600 Loss 3.3058\n",
      "Epoch 4 Batch 4650 Loss 3.3058\n",
      "Epoch 4 Batch 4700 Loss 3.3059\n",
      "Epoch 4 Batch 4750 Loss 3.3057\n",
      "Epoch 4 Batch 4800 Loss 3.3057\n",
      "Epoch 4 Batch 4850 Loss 3.3055\n",
      "Epoch 4 Batch 4900 Loss 3.3056\n",
      "Epoch 4 Batch 4950 Loss 3.3058\n",
      "Epoch 4 Batch 5000 Loss 3.3060\n",
      "Epoch 4 Batch 5050 Loss 3.3062\n",
      "Epoch 4 Batch 5100 Loss 3.3062\n",
      "Epoch 4 Batch 5150 Loss 3.3063\n",
      "Epoch 4 Batch 5200 Loss 3.3064\n",
      "Epoch 4 Batch 5250 Loss 3.3066\n",
      "Epoch 4 Batch 5300 Loss 3.3066\n",
      "Epoch 4 Batch 5350 Loss 3.3066\n",
      "Epoch 4 Batch 5400 Loss 3.3067\n",
      "Epoch 4 Batch 5450 Loss 3.3069\n",
      "Epoch 4 Batch 5500 Loss 3.3069\n",
      "Epoch 4 Batch 5550 Loss 3.3069\n",
      "Epoch 4 Batch 5600 Loss 3.3069\n",
      "Epoch 4 Batch 5650 Loss 3.3071\n",
      "Epoch 4 Batch 5700 Loss 3.3070\n",
      "Epoch 4 Batch 5750 Loss 3.3070\n",
      "Epoch 4 Batch 5800 Loss 3.3070\n",
      "Epoch 4 Batch 5850 Loss 3.3069\n",
      "Epoch 4 Batch 5900 Loss 3.3069\n",
      "Epoch 4 Batch 5950 Loss 3.3069\n",
      "Epoch 4 Batch 6000 Loss 3.3071\n",
      "Epoch 4 Batch 6050 Loss 3.3072\n",
      "Epoch 4 Batch 6100 Loss 3.3072\n",
      "Epoch 4 Batch 6150 Loss 3.3072\n",
      "Epoch 4 Batch 6200 Loss 3.3072\n",
      "Epoch 4 Batch 6250 Loss 3.3071\n",
      "Epoch 4 Batch 6300 Loss 3.3072\n",
      "Epoch 4 Batch 6350 Loss 3.3071\n",
      "Epoch 4 Batch 6400 Loss 3.3072\n",
      "Epoch 4 Batch 6450 Loss 3.3071\n",
      "Epoch 4 Batch 6500 Loss 3.3072\n",
      "Epoch 4 Batch 6550 Loss 3.3073\n",
      "Epoch 4 Batch 6600 Loss 3.3073\n",
      "Epoch 4 Batch 6650 Loss 3.3074\n",
      "Epoch 4 Batch 6700 Loss 3.3073\n",
      "Epoch 4 Batch 6750 Loss 3.3073\n",
      "Epoch 4 Batch 6800 Loss 3.3074\n",
      "Epoch 4 Batch 6850 Loss 3.3074\n",
      "Epoch 4 Batch 6900 Loss 3.3073\n",
      "Epoch 4 Batch 6950 Loss 3.3073\n",
      "Epoch 4 Batch 7000 Loss 3.3073\n",
      "Epoch 4 Batch 7050 Loss 3.3073\n",
      "Epoch 4 Batch 7100 Loss 3.3073\n",
      "Epoch 4 Batch 7150 Loss 3.3074\n",
      "Epoch 4 Batch 7200 Loss 3.3074\n",
      "Epoch 4 Batch 7250 Loss 3.3074\n",
      "Epoch 4 Batch 7300 Loss 3.3072\n",
      "Epoch 4 Batch 7350 Loss 3.3072\n"
     ]
    }
   ],
   "source": [
    "#EPOCHS = 20  \n",
    "EPOCHS = 4  # 1epoch가 매우 오래 걸립니다. \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = music_transformer(inp, True, None, None, None)\n",
    "            loss = loss_function(tar, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, music_transformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, music_transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 제공된 체크포인트 파일을 이용하여 다양한 midi 파일 생성하기\n",
    "\n",
    "- midi 파일을 생성하는 단계에서 바꾸어볼 수 있는 조건에는 무엇이 있는지 찾아보세요\n",
    "- 조건을 변경해가며 5개 이상의 Midi 파일을 생성해보세요.\n",
    "- 가장 잘 생성된 midi 파일을 첨부하여 제출해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "test_dataset = test_dataset.map(tensor_casting)\n",
    "test_dataset = test_dataset.shuffle(10000).batch(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "_inputs = np.zeros([1, N], dtype=np.int32)\n",
    "\n",
    "for x, y in test_dataset.take(1):\n",
    "    _inputs[:, :length] = x[None, :]\n",
    "    \n",
    "for i in range(N - length):\n",
    "    predictions, _ = music_transformer(_inputs[:, i:i+length], False, None, None, None)\n",
    "    predictions = tf.squeeze(predictions, 0)    \n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    print(predicted_id)\n",
    "    \n",
    "    # 예측된 단어를 다음 입력으로 모델에 전달\n",
    "    # 이전 은닉 상태와 함께\n",
    "    _inputs[:, i+length] = predicted_id\n",
    "\n",
    "_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event():\n",
    "    def __init__(self, time, note, cc, on, velocity):\n",
    "        self.time = time\n",
    "        self.note = note\n",
    "        self.on = on\n",
    "        self.cc = cc\n",
    "        self.velocity = velocity\n",
    "\n",
    "    def get_event_sequence(self):\n",
    "        return [self.time, self.note, int(self.on)]\n",
    "\n",
    "class Note():\n",
    "    def __init__(self):\n",
    "        self.pitch = 0\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = []\n",
    "time = 0\n",
    "event = None\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim # 388\n",
    "\n",
    "for _input in _inputs[0]:\n",
    "    # interval\n",
    "    if _input < IntervalDim: \n",
    "        time += _input\n",
    "        event = Event(time, 0, False, 0, 0)\n",
    "\n",
    "    # velocity\n",
    "    elif _input < NoteOnOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.velocity = (_input - VelocityOffset) / VelocityDim * 128\n",
    "\n",
    "    # note on\n",
    "    elif _input < NoteOffOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "\n",
    "        event.note = _input - NoteOnOffset\n",
    "        event.on = True\n",
    "        event_list.append(event)\n",
    "\n",
    "        event = None\n",
    "\n",
    "    # note off\n",
    "    elif _input < CCOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.note = _input - NoteOffOffset\n",
    "        event.on = False\n",
    "        event_list.append(event)\n",
    "        event = None\n",
    "\n",
    "    ## CC\n",
    "    else:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.cc = True\n",
    "        on = _input - CCOffset == 1\n",
    "        event.on = on\n",
    "        event_list.append(event)\n",
    "        event = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import Message, MidiFile, MidiTrack, MetaMessage, bpm2tempo\n",
    "\n",
    "midi = MidiFile()\n",
    "output_midi_path = os.getenv('HOME')+'/aiffel/music_transformer/data/output_file.mid'\n",
    "\n",
    "# Instantiate a MIDI Track (contains a list of MIDI events)\n",
    "track = MidiTrack()\n",
    "track.append(MetaMessage(\"set_tempo\", tempo=bpm2tempo(120)))\n",
    "# Append the track to the pattern\n",
    "midi.tracks.append(track)\n",
    "\n",
    "prev_time = 0\n",
    "pitches = [None for _ in range(128)]\n",
    "for event in event_list:\n",
    "    tick = (event.time - prev_time) // 3\n",
    "    midi.ticks_per_beat = 8\n",
    "    prev_time = event.time\n",
    "\n",
    "    # case NOTE:\n",
    "    if not event.cc:\n",
    "        if event.on:\n",
    "            if pitches[event.note] is not None:\n",
    "                # Instantiate a MIDI note off event, append it to the track\n",
    "                off = Message('note_off', note=event.note, velocity=0, time=0)\n",
    "                track.append(off)\n",
    "                pitches[event.note] = None\n",
    "\n",
    "            # Instantiate a MIDI note on event, append it to the track\n",
    "            on = Message('note_on', note=event.note, velocity=int(event.velocity), time=tick)\n",
    "            track.append(on)\n",
    "            pitches[event.note] = prev_time\n",
    "        else:\n",
    "            # Instantiate a MIDI note off event, append it to the track\n",
    "            off = Message('note_off', note=event.note, velocity=0, time=tick)\n",
    "            track.append(off)\n",
    "            pitches[event.note] = None\n",
    "\n",
    "#     case CC:\n",
    "    elif event.cc:\n",
    "        if event.on:\n",
    "            cc = Message('control_change', control=64, time=tick, value=127)\n",
    "        else:\n",
    "            cc = Message('control_change', control=64, time=tick, value=0)\n",
    "\n",
    "        track.append(cc)\n",
    "\n",
    "    for pitch in range(128):\n",
    "        if pitches[pitch] is not None and pitches[pitch] + 100 < prev_time:\n",
    "            off = Message('note_off', note=pitch, velocity=0, time=0)\n",
    "            track.append(off)\n",
    "            pitches[pitch] = None\n",
    "\n",
    "\n",
    "# Add the end of track event, append it to the track\n",
    "track.append(MetaMessage(\"end_of_track\"))\n",
    "\n",
    "# Save the pattern to disk\n",
    "midi.save(output_midi_path)\n",
    "\n",
    "for i, track in enumerate(midi.tracks):\n",
    "    print('Track {}: {}'.format(i, track.name))\n",
    "    for msg in track:\n",
    "        print(msg)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
